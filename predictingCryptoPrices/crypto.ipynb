{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "from ast import main\n",
    "import os\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING CONSTANTS\n",
    "SEQ_LEN = 60 # get data every 60 mins\n",
    "FUTURE_PERIOD_PREDICT = 3 # predict the next three minutes\n",
    "RATIO_TO_PREDICT = \"BCH-USD\" # setting ratio to predict\n",
    "EPOCHS = 10 # setting number of epochs\n",
    "BATCH_SIZE = 64 # setting batch size\n",
    "NAME = f\"{RATIO_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\" # create unique name for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TARGETS (LABELS) FOR DATA\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current): # check if price is higher in future than in current training data\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING DATA\n",
    "def preprocessDf(df): \n",
    "    df = df.drop(\"future\", 1) # drop future column - makes model redundant... (only needed for generating targets in classify())\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != \"targets\":\n",
    "            df[col] = df[col].pct_change() # normalizing data\n",
    "            df.dropna(inplace = True) # clean the data (remove na)\n",
    "            df[col] = preprocessing.scale(df[col].values) # scale data to get it between 0 to 1\n",
    "\n",
    "    df.dropna(inplace = True) # clean the data (remove na)\n",
    "\n",
    "    sequentialData = [] # create new list to hold sequential data\n",
    "    prevDays = deque(maxlen = SEQ_LEN) # as the list reaches the length, SEQ_LEN, pop out the 'old' items\n",
    "\n",
    "    for i in df.values: # converting datafram to a 'list of list'\n",
    "        prevDays.append([n for n in i[:-1]]) # add all columns up to \"targets\" column\n",
    "        if len(prevDays) == SEQ_LEN:\n",
    "            sequentialData.append([np.array(prevDays), i[-1]]) # add in current label\n",
    "\n",
    "    np.random.shuffle(sequentialData) # shuffle sequences (could remove np)\n",
    "\n",
    "    buys = [] # initalize list, buys\n",
    "    sells = [] # initalize list, sells\n",
    "\n",
    "    for seq, target in sequentialData:\n",
    "        if target == 0: # check if we should sell\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1: # check if we should buy\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    random.shuffle(buys) # shuffle list data\n",
    "    random.shuffle(sells) # shuffle list data\n",
    "\n",
    "    lower = min(len(buys), len(sells)) # get the minimum value of the two lists\n",
    "\n",
    "    buys = buys[:lower] # get what the buys are go up to\n",
    "    sells = sells[:lower] # get what the sells are go up to\n",
    "\n",
    "    sequentialData = buys + sells # put sequential data back together\n",
    "\n",
    "    random.shuffle(sequentialData) # ensure data is not all buys or all sells\n",
    "\n",
    "    # split into features and labels\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequentialData:\n",
    "        x.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(x), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECTING DATA\n",
    "mainDf = pd.DataFrame() # join all dataframes and put them here\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"] # store ratios in a array\n",
    "\n",
    "for ratio in ratios:\n",
    "    dataset = f'crypto_data/{ratio}.csv'\n",
    "    \n",
    "\n",
    "    df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]) # reading data from csv\n",
    "    df.rename(columns = {\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace = True) # rename columns - eliminates error when joining dataframes\n",
    "    \n",
    "    df.set_index(\"time\", inplace = True) # setting index\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]] # creating dataframe\n",
    "\n",
    "    if len(mainDf) == 0: # mainDf is currently empty\n",
    "        mainDf = df # just make mainDf the dataframe made\n",
    "    else:\n",
    "        mainDf = mainDf.join(df) # mainDf wasn't empty so join dataframe to existing dataframes\n",
    "\n",
    "mainDf['future'] = mainDf[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT) # future price is based on close column\n",
    "mainDf['targets'] = list(map(classify, mainDf[f\"{RATIO_TO_PREDICT}_close\"], mainDf[\"future\"])) # mapping data to targets (needed for model)\n",
    "\n",
    "# print(mainDf[[f\"{RATIO_TO_PREDICT}_close\", \"future\", \"targets\"]].head(10)) # check if targets are assigned properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING AND TESTING DATASET\n",
    "times = sorted(mainDf.index.values) # ensure data is sorted by time\n",
    "last5Pc = times[-int((0.05)*len(times))] # timestamp of last 5% of data (threshold to seperate data by)\n",
    "\n",
    "validationMainDf = mainDf[(mainDf.index >= last5Pc)] # obtain testing data where data's time >= threshold timestamp\n",
    "mainDf = mainDf[(mainDf.index < last5Pc)] # obtain training data where data's time >= threshold timestamp\n",
    "\n",
    "# get data sets\n",
    "train_x, train_y = preprocessDf(mainDf) # get training data set (features, labels)\n",
    "validation_x, validation_y = preprocessDf(validationMainDf) # get testing data set (features, labels)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\") # testing to ensure datasets were created properly\n",
    "print(f\"Dont buys: {train_y.count(0)} buys: {train_y.count(1)}\") # testing to ensure datasets were created properly\n",
    "print(f\"VALIDATION dont buys: {validation_y.count(0)} buys: {validation_y.count(1)}\") # testing to ensure datasets were created properly\n",
    "\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "validation_x = np.asarray(validation_x)\n",
    "validation_y = np.asarray(validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING MODEL\n",
    "model = Sequential() # creating a sequential model\n",
    "model.add(LSTM(128, input_shape = (train_x.shape[1:]), return_sequences = True)) # 128 nodes in input layer\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape = (train_x.shape[1:]), return_sequences = True)) \n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape = (train_x.shape[1:]))) # creating dense layer\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation = \"tanh\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = \"softmax\")) # final dense layer: binary choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL AND PREP FOR TRAINING\n",
    "optim = tf.keras.optimizers.Adam(lr = 0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "            optimizer = optim,\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f'logs/{NAME}') # create callback, Tensorboard object\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_accuracy:.3f}\"\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    validation_split = 0.1,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (validation_x, validation_y),\n",
    "    callbacks = [tensorboard, checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "099c61ee22483bbd1374bc1b4f9eb88a902daa72d8017615877b6cb3b2629d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
